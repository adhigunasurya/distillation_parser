{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression building\n",
    "\n",
    "### (note: may have old API in some cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pycnn import *\n",
    "\n",
    "## ==== Create a new computation graph\n",
    "# (it is a singleton, we have one at each stage.\n",
    "# renew_cg() clears the current one and starts anew)\n",
    "renew_cg()\n",
    "\n",
    "## ==== Creating Expressions from user input / constants.\n",
    "x = scalarInput(value)\n",
    "\n",
    "v = vecInput(dimension)\n",
    "v.set([1,2,3])\n",
    "\n",
    "z = matInput(dim1, dim2)\n",
    "\n",
    "# for example:\n",
    "z1 = matInput(2, 2)\n",
    "z1.set([1,2,3,4])\n",
    "\n",
    "## ==== We can take the value of an expression. \n",
    "# For complex expressions, this will run forward propagation.\n",
    "print z.value()    \n",
    "print z.npvalue()      # as numpy array\n",
    "print v.vec_value()    # as vector, if vector\n",
    "print x.scalar_valur() # as scalar, if scalar\n",
    "print x.value()        # choose the correct one\n",
    "\n",
    "## ==== Parameters\n",
    "# Parameters are things we tune during training.\n",
    "# Usually a matrix or a vector.\n",
    "\n",
    "# First we create a model and add the parameters to it.\n",
    "m = Model() \n",
    "pW = m.add_parameters(\"W\", (8,8)) # an 8x8 matrix\n",
    "pb = m.add_parameters(\"b\", 8)\n",
    "\n",
    "# then we create an Expression out of the model's parameters\n",
    "W = parameter(pW) # or W = parameter(m[\"W\"])\n",
    "b = parameter(pb)\n",
    "\n",
    "## ===== Lookup parameters\n",
    "# Similar to parameters, but are representing a \"lookup table\"\n",
    "# that maps numbers to vectors.\n",
    "# These are used for embedding matrices.\n",
    "# for example, this will have VOCAB_SIZE rows, each of DIM dimensions.\n",
    "lp  = m.add_lookup_parameters(\"lookup\", (VOCAB_SIZE, DIM))\n",
    "\n",
    "# lookup parameters can be initialized from an existing array, i.e:\n",
    "# m[\"lookup\"].init_from_array(wv)\n",
    "\n",
    "e5  = lookup(lp, 5)   # create an Expression from row 5.\n",
    "e5c = lookup(lp, 5, update=False)  # as before, but don't update when optimizing.\n",
    "e5.set(9)   # now the e5 expression contains row 9\n",
    "e5c.set(9)  # ditto\n",
    "\n",
    "## ===== Combine expression into complex expressions.\n",
    "\n",
    "# Math \n",
    "e = e1 + e2   \n",
    "e = e1 * e2   # for vectors/matrices: matrix multiplication (like e1.dot(e2) in numpy)\n",
    "e = e1 - e2    \n",
    "e = -e1 \n",
    "\n",
    "e = dot_product(e1, e2)\n",
    "e = cwise_multiply(e1, e2)  # component-wise divide  (like e1*e2 in numpy)\n",
    "e = cdiv(e1, e2)            # component-wise divide\n",
    "e = colwise_add(e1, e2)     # column-wise addition\n",
    "\n",
    "# Matrix Shapes\n",
    "e = reshape(e1, new_dimension)\n",
    "e = transpose(e1)\n",
    "\n",
    "# Per-element unary functions.\n",
    "e = tanh(e1)      \n",
    "e = exp(e1)\n",
    "e = log(e1)\n",
    "e = logistic(e1)   # Sigmoid(x)\n",
    "e = rectify(e1)    # Relu (= max(x,0))\n",
    "e = softsign(e1)    # x/(1+|x|)\n",
    "\n",
    "# softmaxes\n",
    "e = softmax(e1)\n",
    "e = log_softmax(e1, restrict=[]) # restrict is a set of indices. \n",
    "                                 # if not empty, only entries in restrict are part \n",
    "                                 # of softmax computation, others get 0.\n",
    "\n",
    "\n",
    "e = sum_cols(e1)\n",
    "\n",
    "\n",
    "# Picking values from vector expressions\n",
    "e = pick(e1, k)              # k is unsigned integer, e1 is vector. return e1[k]\n",
    "e = e1[k]                    # same\n",
    "\n",
    "e = pickrange(e1, k, v)      # like python's e1[k:v] for lists. e1 is an Expression, k,v integers.\n",
    "e = e1[k:v]                  # same\n",
    "\n",
    "e = pickneglogsoftmax(e1, k) # k is unsigned integer. equiv to: (pick(-log(softmax(e1)), k))\n",
    "                             \n",
    "\n",
    "# Neural net stuff\n",
    "noise(e1, stddev) # add a noise to each element from a gausian with standard-dev = stddev\n",
    "dropout(e1, p)    # apply dropout with probability p \n",
    "\n",
    "# functions over lists of expressions\n",
    "e = esum([e1, e2, ...])            # sum\n",
    "e = average([e1, e2, ...])         # average\n",
    "e = concatenate_cols([e1, e2, ...])  # e1, e2,.. are column vectors. return a matrix. (sim to np.hstack([e1,e2,...])\n",
    "e = concatenate([e1, e2, ...])     # concatenate\n",
    "\n",
    "e = affine_transform([e0,e1,e2, ...])  # e = e0 + ((e1*e2) + (e3*e4) ...) \n",
    "\n",
    "## Loss functions\n",
    "e = squared_distance(e1, e2)\n",
    "e = l1_distance(e1, e2)\n",
    "e = huber_distance(e1, e2, c=1.345)\n",
    "\n",
    "# e1 must be a scalar that is a value between 0 and 1\n",
    "# e2 (ty) must be a scalar that is a value between 0 and 1\n",
    "# e = ty * log(e1) + (1 - ty) * log(e1)\n",
    "e = binary_log_loss(e1, e2)\n",
    "\n",
    "# e1 is row vector or scalar\n",
    "# e2 is row vector or scalar\n",
    "# m is number\n",
    "# e = max(0, m - (e1 - e2))\n",
    "e = pairwise_rank_loss(e1, e2, m=1.0) \n",
    "\n",
    "# Convolutions    TODO\n",
    "e = conv1d_narrow(e1, e2) #\n",
    "e = conv1d_wide(e1, e2)   #\n",
    "e = kmax_pooling(e1, k) #  kmax-pooling operation (Kalchbrenner et al 2014)\n",
    "e = kmh_ngram(e1, k) # \n",
    "e = fold_rows(e1, nrows=2) #\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added\n",
      "[1, 2, 3] 1\n",
      "3.28097844124\n",
      "[3, 2, 4] 2\n",
      "1.66672432423\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from pycnn import *\n",
    "\n",
    "# create model\n",
    "m = Model()\n",
    "\n",
    "# add parameters to model\n",
    "m.add_parameters(\"W\", (10,30))\n",
    "m.add_parameters(\"b\", 10)\n",
    "m.add_lookup_parameters(\"lookup\", (500, 10))\n",
    "print \"added\"\n",
    "\n",
    "# create trainer \n",
    "trainer = SimpleSGDTrainer(m)\n",
    "\n",
    "# L2 regularization and learning rate parameters can be passed to the trainer:\n",
    "# alpha = 0.1  # learning rate\n",
    "# lambda = 0.0001  # regularization\n",
    "# trainer = SimpleSGDTrainer(m, lam=lambda, e0=alpha)\n",
    "\n",
    "# function for graph creation\n",
    "def create_network_return_loss(model, inputs, expected_output):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    renew_cg()\n",
    "    W = parameter(model[\"W\"])\n",
    "    b = parameter(model[\"b\"])\n",
    "    lookup = model[\"lookup\"]\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = concatenate(emb_vectors)\n",
    "    net_output = softmax( (W*net_input) + b)\n",
    "    loss = -log(pick(net_output, expected_output))\n",
    "    return loss\n",
    "\n",
    "# function for prediction\n",
    "def create_network_return_best(model, inputs):\n",
    "    \"\"\"\n",
    "    inputs is a list of numbers\n",
    "    \"\"\"\n",
    "    renew_cg()\n",
    "    W = parameter(model[\"W\"])\n",
    "    b = parameter(model[\"b\"])\n",
    "    lookup = model[\"lookup\"]\n",
    "    emb_vectors = [lookup[i] for i in inputs]\n",
    "    net_input = concatenate(emb_vectors)\n",
    "    net_output = softmax( (W*net_input) + b)\n",
    "    return np.argmax(net_output)\n",
    "\n",
    "\n",
    "# train network\n",
    "for inp,lbl in ( ([1,2,3],1), ([3,2,4],2) ):\n",
    "    print inp, lbl\n",
    "    loss = create_network_return_loss(m, inp, lbl)\n",
    "    print loss.value() # need to run loss.value() for the forward prop\n",
    "    loss.backward()\n",
    "    trainer.update()\n",
    "\n",
    "print create_network_return_best(m, [1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
